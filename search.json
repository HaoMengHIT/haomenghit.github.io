[{"title":"论文阅读：Using Performance-Power Modeling to Improve Energy Efficiency of HPC Applications （IEEE Computer 2016）","url":"/2019/08/05/论文阅读：Using-Performance-Power-Modeling-to-Improve-Energy-Efficiency-of-HPC-Applications/","content":"\n## 摘要\n\n要在HPC系统上开发节能应用程序，了解运行时、功耗和每个应用程序的独特特性之间的关系非常重要。在本文中，我们提出了一个功率和性能建模和节能框架，并使用此框架来模拟运行时、系统功率、CPU功率和内存功率。组合模型的性能计数器与应用程序信息结合使用，可以为识别可能降低能耗的应用程序修改提供基础。基于这些模型，我们开发了一个基于网页的假设预测系统，从理论上预测可能的优化结果。我们通过两个超级计算机Mira和SystemG 对两个应用程序，地震模拟和航空航天应用进行了验证。我们的计数器引导优化方法实现Mira上32,768个核心的能耗平均降低了48.65％，SystemG上256个核心的能耗降低了30.67％。\n\n## 研究背景\n\nHPC系统，特别是千万亿次超级计算机，目前消耗了大量的电力。目前500强名单中的前五大系统总耗电量超过50兆瓦，平均功耗为10兆瓦，平均性能为17.54千万亿次（http://www.top500.org/lists/2015/11/）。考虑到实现具有20MW功率的百亿亿次系统的愿望，人们认识到这样的系统将受到功率和能量消耗的极大限制，并且它需要独特的方法来平衡功率和性能。为此，理解运行时、功耗和每个大规模科学应用的独特特征（例如，循环结构，数据结构，数据移动，通信重叠，同步等）之间的关系是很重要的。关于这些关系的见解为应用程序优化提供了指导，以降低功率和能耗。优化可能涉及应用程序修改、系统调整或两者的组合。在本文中，我们将探讨这些组合。\n\n有许多应用程序优化方法可以减少运行时间，例如算法优化、循环嵌套优化、编译器优化技术等。还有许多编程模型、语言和运行时系统，如果经过深思熟虑，也可以减少运行时间。\n\n大多数供应商在硬件级别使用功率监视技术来动态地降低各种资源的功耗，简单的像旋转磁盘和降低空闲核心，复杂的实现异步时钟电路。如今，几乎所有微处理器都包含许多动态资源分配技术，以节省电力，同时按需提供性能。\n\n此外，还有***两种基于软件的技术可以降低任意工作负载的功耗。第一种是动态电压和频率调节（DVFS），其中根据策略和需求在一些时间窗口内动态调整CPU频率。另一种是动态并发限制（DCT），这种技术可以在类似的约束条件下在运行时调整并发级别。***\n\n***直观地节省能量意味着降低功率或减少运行时间，或两者兼而有之***。因此，该领域的所有研究可分为三类：（1）减少时间和功率;（2）在牺牲增加功率的同时减少时间;（3）在牺牲时间增加的情况下降低功率。功耗（$E$）是时间（$T$）范围的平均功率（$W$：平均功率）的能量，即$E = T * W$。为了阐明这三个类别，我们假设时间的百分比变化是 $a(0 <a <1)$，并且与进行修改的基线相比，功耗的百分比变化是 $b（0 <b <1）$。下面，我们提供三个类别的简单数学分析。\n\n（1）同时降低时间和功率\n\n假设时间降为 $T*(1-a)$,功率降为 $W*(1-b)$ ,那么相应的能耗降为$T*(1-a)*W*(1-b) = (1-a)(1-b)*T*W<T*W$,在这种情况下，能耗节省了 $1-(1-a)(1-b)=a+b-ab$ 。\n\n在[3]中，作者使用DVFS和DCT通过同时节能（平均6％）和性能改进（平均14％）实现了能量显着降低（平均19％）。在[6]中采用了类似的方法，平均节能4.18％，性能提升高达7.2％。在我们之前的工作[8]中，使用DVFS，DCT和代码修改，我们能够将运行时间降低多达14.15％并同时将功耗降低多达12.50％。\n\n（2）降低时间，增加功率\n\n假设减少的时间是 $T*(1-a)$，并且增加的功耗是 $W*(1+b)$ ，我们得到的能量：$T*(1-a)*W*(1+b)=(1-a)(1+b)* T* W$。如果 $(1-a)(1+b)*T*W <T*W$ ，即 $b-a-ab <0$，则发生能量减少。如果 $b≤a$，则发生节能。这种方法在通过资源利用率的增加（例如，并发性的增加）来减少运行时的方法中是常见的。\n\n（3）降低功率，增加时间\n\n假设时间增加到 $T*(1+a)$, 功率减少到 $W*(1-b)$，最终的能耗为 $T*(1+a)*W*(1-b)=(1+a)(1-b)*T*W$。如果 $(1+a)(1-b)*T*W<T*W$，也就是 $a-b-ab<0$，则能耗降低。如果 $a≤b$，则发生节能。这种方法在使用DVFS的方法中很常见。***理论上大多数应用程序都花费时间等待CPU之外的其他所有事情，因此降低CPU频率可以比性能成本节省更多功率。***这一概念促成了IBM（BlueGene），SiCortex（MIPS）和Calxeda（ARM）等供应商开发的新架构。\n\n在本文中，我们使用性能和功率建模方法，利用前两个类别来指导节能应用程序的开发，因为BG/Q只有固定的CPU频率设置（1.6GHz）。\n\n## 性能和功率建模\n\n有必要准确地测量或估计功耗。由于高频下的直接在线功率测量是不切实际的，因此硬件性能计数器已被广泛用作估计功耗的有效代理。硬件性能计数器已经包含在大多数现代架构中，并且暴露在商业硬件上的用户空间中。性能计数器通过计算特定事件（如高速缓存未命中、流水线停顿、浮点运算、字节输入/输出、字节读/写等）来监视系统组件（如处理器、内存、网络和I/O）。可以在硬件级别收集此类事件的统计信息，几乎没有开销。这使得性能计数器成为监视应用程序，分析其硬件资源使用情况以及估计其运行时和功耗的有力手段。\n\n以前关于功率建模和估计的大部分工作都是基于性能计数器[5,4,2,11,7,1,10,8,13,17]。这些方法使用性能计数器来监视多个系统组件。然后，作者尝试将这些数据与每个系统组件消耗的功率相关联。该相关性用于推导可以估计每个系统组件的功耗的模型。其结果的准确性取决于性能计数器的选择/可用性，评估的基准/应用以及所使用的特定统计数据拟合方法。许多上述方法使用一小组性能计数器（通常少于10个计数器）进行功率建模。在我们最近的工作[8]中，我们针对指标开发了四种不同的模型：基于40个性能计数器的运行时，系统功耗，CPU功率和内存功率。我们发现用于每个不同模型的性能计数器并不相同。在研究六种科学应用时，我们发现共有37种不同的性能计数器用于模型，从模型到模型只有3或4个计数器是相同的。\n\n**为了开发运行时和功耗模型，我们在一个系统上收集了40个可用的性能计数器，这些计数器具有不同的系统配置（核心数，节点数）和应用程序问题大小。然后，我们使用Spearman相关和主成分分析（PCA）来识别主要性能计数器($r_1,r_2,\\cdots r_n (n<40)$)，这些计数器与度量运行时、系统功率、CPU功率或内存功率高度相关。然后我们使用非负多元回归分析来生成基于一小部分主要计数器和CPU频率（$f$）的四个模型。**\n\n对运行时间$t$建模如下：\n$$\nt=\\beta_0+\\beta_1*r_1+\\cdots+\\beta_n*r_n+\\beta*\\frac{1}{f} \\tag{1}\n$$\n在这里，$\\beta_0$是截距，$\\beta_n$表示性能计数器$r_n$的回归系数，$\\beta$是CPU频率$f$的系数。\n\n类似地，我们可以使用以下等式对CPU功耗p进行建模：\n$$\np=\\alpha_0+\\alpha_1*r_1+\\cdots +\\alpha_n*r_n+\\alpha*f^3  \\tag{2}\n$$\n在这里，$\\alpha_0$是截距，$\\alpha_n$表示性能计数器$r_n$的回归系数， $\\alpha$是CPU频率$f$的系数。系统功率和存储器功率模型的等式与等式2类似。\n\n## 建模与节能框架\n\n图1是我们基于计数器的建模和节能框架的功能的总图。我们使用MuMMI [16]收集性能计数器以及我们希望关联的四个指标，并将数据上传到MuMMI数据库。所有性能计数器都根据执行的总周期数进行标准化，以便为每个计数器创建性能事件率。接下来，执行Spearman相关和主成分分析（PCA）以识别与四个指标相关的重要计数器。然后，使用等式1和2，基于减少的计数器组和CPU频率，使用非负多元回归分析来生成四个模型中的每一个。我们之前的工作表明运行时和功率模型对于所使用的六种科学应用，预测误差率平均低于7％。MuMMI提供基于Web的建模系统，可根据计数器的数据和MuMMI数据库中的四个指标自动生成运行时和功率模型。\n\n{% asset_img Figure1.png [20] [20]%}\n\n在指标的四个模型的基础上，我们实施了一个计数器排名方法，以确定哪些测量计数器做出了重大贡献。然后，这些计数器用于指导应用程序修改，以实现运行时和功耗的减少。\n\n#### 计数器相关性分析与排序\n\n一旦我们获得了运行时、系统功耗、CPU功率和内存功率的模型，我们就会确定四个模型中每个模型的最重要的性能计数器。\n\n{% asset_img Figure2.png [20] [20]%}\n\n排名算法如图2所示，其工作原理如下：首先，创建计数器列表，其由具有最高系数百分比的计数器组成。根据运行时间、系统功率、CPU功率和存储器功率的顺序，这些计数器的系数与四个模型的所有系数之和的比率最高。其次，按照相同的顺序，我们从计数器列表中删除无效的计数器（小于1％的计数器）。最后，我们使用成对的Spearman相关性来分析和排序这些计数器之间的相关性，以识别对模型最有效贡献的计数器，以形成最终的计数器列表。进行修剪使得如果具有较高等级的计数器与较低等级的计数器高度相关，则消除具有较低等级的计数器。结果列表的计数器用于指导应用程序修改。\n\n#### 节能策略推荐\n\n众所周知，性能计数器的值与影响性能和功耗的应用程序的属性相关。许多代码优化仅专注于改进缓存重用以减少应用程序运行时间，因为已知内存访问是大多数体系结构的主要瓶颈。但是，这些努力通常基于来自几次运行的性能数据，而很少考虑数据依赖性，问题大小和/或系统配置。此外，他们倾向于忽视功耗问题。在这项工作中，性能和功耗模型是根据不同的系统配置和问题大小生成的，因此可以广泛了解应用程序对底层架构的使用。这样可以更好地了解应用程序在给定体系结构上的能耗。例如，如果我们将计数器 $r_1$ 和 $r_2$ 作为最重要的计数器，其中 $r_1$ 在运行时模型中占主导地位，而 $r_2$ 在功率模型中占主导地位，并且发现两者都是不相关的，那么我们对应用程序的修改将集中在两个计数器上。通过这种方式，我们的修改不仅可以减少应用程序运行时间，还可以降低功耗。但是，使用通用、功能无意识的性能工具，如gprof，TAU，ScoreP，HPCToolkit，HPM Toolkit和CrayPat，对计数器 $r_2$ 的影响也许完全不被注意。\n\n请考虑以下事项：假设$r_i$，$r_j$和$r_k$为三个性能计数器，这些计数器对运行时模型（公式1）或功率模型（公式2）（系统、CPU、或内存的功率模型）有着显著的贡献。其中$r_i$被认为是最重要的。$r_i$和$r_j$的相关值为0.9，与$r_k$的相关值为0.6。如果$r_i$的值减少20%，则$r_j$的值降低18%（0.9*20%），$r_k$的值减少12%（0.6 * 20%）。在此假设下，我们使用等式1和2来预测对运行时和功耗的理论影响。\n\n通常，基于运行时和功率模型以及计数器相关性，我们开发了一个基于Web的假设预测系统，用于在理论上预测可能的优化结果，如图3所示，例如，给定应用程序PMLB [15] ，如果TLB_IM的数量减少20％，相关计数器将根据它们与TLB_IM的相关值减少，运行时间减少4.13％，平均节点功率几乎相同，CPU和内存功耗也减少了一点。\n\n{% asset_img Figure3.png [20] [20]%}\n\n然后立即出现一个问题，即如何将计数器$r_i$的值减少20％。这需要彻底了解应用程序特征以及影响该特定度量标准的底层架构部分。在[12]中，讨论了几种典型的代码性能模式，并映射到一些可以帮助代码优化的硬件度量。重要的是要意识到像PAPI预设[9]这样的通用性能计数器可能很容易被用户误解为不同的体系结构。用户必须在体系结构手册中查找确切的定义，并了解应用程序特征和底层体系结构单元如何影响计数器。\n\n## 案例研究：性能计数器引导的能源优化\n\n在本节中，我们使用两个科学应用：并行航空航天应用PMLB [15]和并行地震模拟eq3dyna [14]，来讨论在两个功率感知超级计算机Mira，和SystemG上的性能计数器引导的能量优化。\n\n#### PMLB on SystemG \n\n图4显示了四个模型的性能计数器排名，使用15个不同的PMLB计数器，SystemG上的问题大小为128x128x128。我们将图2中的排名算法应用于四个模型中的每一个的计数器。将它们从最重要到最小排序，它们是：TLB_IM（指令转换后备缓冲器（TLB）未命中），VEC_INS（向量/ SIMD指令），TLB_DM和L2_ICM。 \n\n{% asset_img Figure4.png [20] [20]%}\n\nTLB_IM排名最高，其次是VEC_INS。我们使用成对的Spearman相关来分析两个计数器之间的相关性，如下所示：\n\n{% asset_img Figure5.png [20] [20]%}\n\n我们发现计数器TLB_IM仅在运行时模型中有贡献，并且它与TLB_DM和L2_ICM相关。然而，VEC_INS在系统功率，CPU功率和存储器功率的模型中有所贡献，并且与任何其他计数器无关。因此，我们专注于对SystemG上的计数器TLB_IM和VEC_INS进行优化。从理论上讲，使用我们的假设预测系统，将TLB_IM的数量减少20％可以使运行时间减少4.13％;将VEC_INS加速20％导致节点功率降低1.85％。\n\nSystemG上的Linux系统支持两种页面大小，默认为4KB，大页面为2MB。单个2MB大页面只需要一个TLB条目，而等量的内存需要使用4KB页面的512个TLB条目。因此，为性能受TLB未命中约束的应用程序启用此类页面大小可能具有显着的益处。在这里，我们使用libhugetlbfs为应用程序执行启用了2MB页面，以减少TLB未命中。我们还对代码进行了矢量化，并使用编译器选项-ftree-loop-distribution来执行循环分配，以提高大循环体上的缓存性能，并允许进行进一步的循环优化，如矢量化。\n\n这些优化的结果如图5所示. SystemG每个节点有8个核心。我们观察到应用程序运行时间平均减少了11.23％（图5（a）），系统功率平均增加了0.01％（图5（b））。图5（c）中的CPU功率平均增加了2.13％，图5（d）中的存储器功率平均增加了0.61％。总的来说，这意味着平均节能11.28％。据观察，平均节能百分比（11.28％）大于运行时间改善百分比（11.23％），这意味着减少运行时间和功率会导致更大的节能。\n\n{% asset_img Figure6.png [20] [20]%}\n\n## 总结\n\n使用我们的基于性能计数器的建模和节能框架，我们开发了一个基于Web的假设预测系统，从理论上预测可能的优化结果。**我们相信这个框架可以应用于在其他架构上执行的大规模科学应用，包括GPGPU等硬件和Intel-Xeon Phi等多核加速器，并且可以使用一个架构上的这些应用的功率和性能模型来预测具有类似架构的大规模系统的功耗和性能。我们的方法代表了一种通用的全面优化方法，专注于最有效地利用可用资源，平衡运行时间与功耗。通过这种方式，我们希望我们的方法能够为应用程序和系统开发人员提供额外的指导，以开发下一代节能应用程序和超级计算机。**","tags":["Papers","Energy Modeling","Journal"]},{"title":"Pareto最优相关介绍","url":"/2019/08/04/Pareto最优相关介绍/","content":"\n## Pareto Optimality（帕累托最优）\n\n帕累托最优是指资源分配的一种理想状态，即假定固有的一群人和可分配的资源，从一种分配状态到另一种状态的变化中，在没有使任何人境况变坏的前提下，也不可能再使某些人的处境变好。换句话说，就是不可能再改善某些人的境况，而不使任何其他人受损。 \n\n## Pareto解\n\n多目标规划中，由于存在目标之间的冲突和无法比较的现象，一个解在某个目标上是最好的，在其他的目标上可能比较差。Pareto 在1986 年提出多目标的解不受支配解(Non-dominated set)的概念。其定义为：假设任何二解S1及S2对所有目标而言，S1均优于S2，则我们称S1 支配S2，若S1没有被其他解所支配，则S1 称为非支配解（不受支配解），也称Pareto解。 \n\n## Pareto Front\n\nPareto解的集合即所谓的Pareto前沿。座落在Pareto front 中的所有解皆不受Pareto Front 之外的解（以及Pareto Front 曲线以内的其它解）所支配，因此这些非支配解较其他解而言拥有最少的目标冲突，可提供决策者一个较佳的选择空间。在某个非支配解的基础上改进任何目标函数的同时，必然会削弱至少一个其他目标函数。\n\n## 参考\n\n[1] Pareto. https://blog.csdn.net/scutwjh/article/details/46129405\n\n","tags":["统计学"]},{"title":"Q-Q图介绍","url":"/2019/08/04/Q-Q图介绍/","content":"\n## 什么是Q-Q图？\n\nQ-Q图（Quantile-Quantile Plots）是一种散点图，横坐标为某一样本的分位数，纵坐标为另一样本的分位数，横坐标与纵坐标组成的散点图代表同一个累计概率所对应的分位数。分位数是某些值低于该分位数的值，例如，中位数是一个分位数，其中50%的数据低于该点，50%高于该点。Q-Q图的目的是确定两组数据是否来自同一分布。在Q-Q图上绘制45度角;如果两个数据集来自共同分布，则这些点将落在该参考线上。\n\n{% asset_img Figure1.png %}\n\n上图显示了水平轴上理论正态分布的分位数。它与y轴上的一组数据进行比较。这种特殊类型的Q-Q图称为常规Q-Q图。这些点没有聚集在45度线上，实际上遵循曲线，表明样本数据不是正态分布的。\n\n## 如何创建Q-Q图？\n\n示例问题：以下数据是否来自正态分布？\n\n7.19, 6.31, 5.89, 4.5, 3.77, 4.25, 5.19, 5.79, 6.79.\n\n（1） 步骤1：将数据从小到大排序。\n\n3.77， 4.25，4.50，5.19，5.89，5.79，6.31，6.79，7.19\n\n（2）步骤2：绘制正态分布曲线。将曲线分成n + 1个段。我们有9个值，因此将曲线分成10个大小相等的区域。对于此示例，每个段是区域的10％（因为100％/ 10 = 10％）。\n\n{% asset_img Figure2.png %}\n\n（3）步骤3：在该步骤中找到每个段的z值（截止点）。这些段是区域，因此请参考z表（或使用软件）获取每个段的z值。\n\nz值为：\n\n- 10% = -1.28\n\n- 20% = -0.84\n\n- 30% = -0.52\n\n- 40% = -0.25\n\n- 50% = 0\n\n- 60% = 0.25\n\n- 70% = 0.52\n\n- 80% = 0.84\n\n- 90% = 1.28\n\n- 100% = 3.0\n\n  \n\n{% asset_img Figure3.png %}\n\n（4）第4步：根据正态分布截止点绘制数据集值（步骤1）（步骤3）：\n\n{% asset_img Figure4.png %}\n\n此Q-Q图上的数据几乎是一条直线，表示数据大致满足正态分布。\n\n**注意：**\n\n> 此示例使用标准正态分布，但如果认为你的数据可能来自不同的正态分布（即具有不同均值和标准偏差的分布），那么你可以使用它。\n>\n> 对于许多统计测试来说，正态性假设是一个重要的假设；假设你是从正态分布的人群中取样的。正态Q-Q图是评价正态性的一种方法。但是，你不必使用正态分布作为数据的比较；你可以使用任何连续分布作为比较（例如威布尔分布或均匀分布），只要你可以计算分位数。\n\n## 参考\n\n[1] Q-Q Plots: Simple Definition & Example.  https://www.statisticshowto.datasciencecentral.com/q-q-plots","tags":["统计学"]},{"title":"论文阅读：Energy Efficiency Modeling of Parallel Applications (SC18)","url":"/2019/08/03/论文阅读：Energy-Efficiency-Modeling-of-Parallel-Applications-SC18/","content":"\n## 摘要\n\n随着功率限制和成本的增加，能效在高性能计算（HPC）中变得越来越重要。工作负载和系统特性构成了一个复杂的优化搜索空间，其中能效和性能的最佳设置通常不同。因此，我们必须确定性能和能效的权衡选择，以找到它们之间的理想平衡。我们提出了一种创新的统计模型，该模型仅使用用户可控参数准确预测帕累托最优性能和能效折衷选项。我们的方法也可以容忍测量和模型误差。我们使用多个HPC内核研究模型训练和验证，然后探索将模型应用于更复杂的工作负载（包括AMG和LAMMPS）的可行性。我们可以从最少12次运行校准精确模型，预测误差小于10％。我们的结果确定了权衡选项，能够以低于20％的性能损失为代价，将能效提高40％。对于AMG，我们将所需的样品测量时间从13小时减少到74分钟（约90％）。\n\n## 研究背景\n\n目前，功率限制限制了超级计算机中CPU核心和计算节点的数量。我们必须开发创新方法来预测并行计算的性能和能源使用之间的关系。根据情况，在应用开发和部署中，性能和能效之间的可用权衡选择至关重要。系统资源管理必须通过在运行时正确配置应用程序来最大限度地满足给定功率预算。此外，程序员需要实用的工具来有效地识别和选择权衡选项，以优化其并行应用程序的能源效率。\n\n不幸的是，许多因素影响了权衡。这些因素包括应用程序特征，如计算强度，内存和通信访问模式，以及系统因素，如缓存设计，内存和网络带宽。因此，优化这种多目标问题的工具[4]必须搜索大而复杂的空间。\n\n***帕累托边界通常代表能源使用与性能之间的权衡选择***。因此，可以通过预测帕累托前沿的最佳配置的模型来估计权衡选项。我们提出了一种回归建模技术，可以预测帕累托最优能量效率和性能权衡选项，但不需要完整的遍历搜索空间。\n\n与现有的基于模型的方法[5]，[6]，[7]，[8]相比，我们的技术具有一些优势，可以提高并行计算的能效。首先，它只需要一组最小的输入变量，超级计算机用户可以自己简单地访问和控制。相比之下，大多数现有模型[5]，[6]建立在几个直接反映能效的运行时测量上，例如计算强度，高速缓存未命中率，停顿周期，存储器访问次数和网络通信量。虽然这些模型提供了有价值的见解，但这些参数对于应用程序用户来说很难控制。获取这些测量值的过程也增加了将这些模型自动化为实用工具的难度。其次，我们的模型在预测的权衡选项中考虑样本测量误差和模型预测误差。第三，大多数现有模型不直接给出帕累托效率配置。\n\n我们证明了我们的模型以低成本准确地预测了混合MPI/OpenMP程序的最佳配置的Pareto前端。我们提出了一项研究，评估代表性并行内核和应用程序模型的拟合和方差。通过研究，我们发现我们的方法和模型可以成功识别与广泛的搜索优化空间方法的测量数据一致的权衡选项。此外，构建我们的模型只需要一小组样本测量，这样可以最大限度地减少训练所需的工作量和资源。总的来说，我们的模型是实用的自动化工具的理想选择。\n\n具体而言，本文提出了以下贡献：\n\n（1）使用并行应用程序用户可以直接控制的参数进行能源和性能权衡;\n\n（2）一种从少量输入测量中预测能效和性能权衡选项的实用方法;\n\n（3）准确预测并行应用响应的多目标能效和性能模型;\n\n（4）在给定测量和/或建模误差的情况下，权衡区方法可改善Pareto优化。\n\n## 研究方法\n\n#### 模型预测器\n\n{% asset_img Figure1.png %}\n\n初始步骤选择模型输入参数或预测器，可以准确地模拟我们的响应：系统能效和性能。之前的工作[29]表明系统特性，工作负载特性，并发性，CPU频率缩放和观察到的系统响应之间存在复杂的关系。我们使用表I中的定义来表达这些关系，如等式1和2所示。\n$$\nE(w_n,s_m)=F_e(n_i,c_j,f_k)\\tag{1}\n$$\n\n$$\nP(w_n,s_m)=F_p(n_i,c_j,f_k)\\tag{2}\n$$\n\n#### 模型形式化\n\n为了制定多元回归模型，我们首先使用节点数量，核心数量和CPU频率设置作为模型输入来分析一组代表性软件内核的响应。观察到的曲线性能和能量效率响应意味着模型需要多项式项。\n\n多项式的不灵活性可以极大地影响基本多项式拟合。它可能导致预测响应中的不期望的振荡，或者由小样本子集过度影响的总响应形状。为了最小化这种不灵活性的影响，我们从使用普通最小二乘回归的B样条分段多项式模型开始。我们发现样条在我们的实验中始终优于基本多项式。\n\n样条函数用于沿一系列点或节点拟合平滑曲线。我们构造了从多项式函数中连接每一对点或样条的曲线。B样条或基本样条函数可改善结的连续性，否则可能会影响模型的连续性。\n\n我们有几种配置选项可以调整模型的准确性和效率。我们考虑包括样条的多项式度和自由度的选项。增加这些模型参数允许样条曲线拟合更复杂的曲线。我们还考虑了项，线性和非线性项之间的相互作用，转换以减少数据分布偏差的影响，以及采样方法。\n\n为了指导模型调整，我们观察每个预测因子在响应中驱动的可变性水平，并使用预测变量之间预期相互作用的知识边缘。例如，增加核心数会驱动产生非线性响应的资源争用。增加CPU频率通常会驱动线性响应，直到节点内的核心争用开始占主导地位。这些观察引导我们进行以下模型设置和简化：\n\n（1）B样条自由度为3;\n\n（2）节点和核心数是二次多项式项;\n\n（3）CPU频率是一个线性项;\n\n（4）节点和核心数有一个交互项;\n\n（5）核心数和频率有互动项;\n\n（6）频率和节点数没有交互项;\n\n（7）响应的自然对数变换。\n\n为了通过实验评估模型设置，我们使用迭代方法来评估数据分布和相关性，模型拟合度和系数量值。例如，我们通过实验将模型拟合与CPU频率作为多项式或线性项进行比较，发现线性项选项在我们的研究测试案例中提供了平均RMS误差的轻微改善（2.3％）。我们还通过实验确定了模型欠拟合和过度拟合之间的过渡点处的最优多项式项顺序和B样条自由度。例如，核心数量和频率交互项的近零系数证实我们可以将其从模型中移除而几乎没有影响。\n\n因此，我们在均匀HPC群集上运行的并行应用的能效和性能统计模型使用公式3和4。\n$$\nlog_e(e_\\eta)\\sim bs(n_i)+bs(c_j)+f_k+bs(c_j):f_k+bs(n_i):bs(c_j)\\tag{3}\n$$\n\n$$\nlog_e(p_\\mu)\\sim bs(n_i)+bs(c_j)+f_k+bs(c_j):f_k+bs(n_i):bs(c_j)\\tag{4}\n$$\n\n这些方程基于Wilkinson符号，它被回归分析工具（如MAT LAB，或R和Python编程库）所接受。所使用的运算符\"~\"表示模型的构成，“+”表示在模型中添加项，“:”表示两项之前的交互关系。\n\n除了能效和性能率之外，模型还可以基于等式5和6来获取例如总能量和总时间的累积响应。\n$$\nlog_e(e_T)\\sim bs(n_i)+bs(c_j)+f_k+bs(c_j):f_k+bs(n_i):bs(c_j)\\tag{5}\n$$\n\n$$\nlog_e(t_T)\\sim bs(n_i)+bs(c_j)+f_k+bs(c_j):f_k+bs(n_i):bs(c_j)\\tag{6}\n$$\n\n我们将自然指数应用于模型预测以反转对数变换。\n\n公式7显示了方程3到6的右侧扩展。每个模型有20个预测项（1个截距，1个线性项，2$\\times$3个样条B样条项，1$\\times$3个样条B样条$\\times$3个样条B- 样条相互作用项，1$\\times$3样条B样条$\\times$1个线性项。我们收集预测因子和响应训练数据，然后使用最小二乘回归来确定各项系数，$\\beta_1$ 到$\\beta_{19}$。\n\n{% asset_img Figure2.png %}\n\n#### 模型评估\n\n为了评估模型，我们使用相关分析，$k$交叉验证，RMS误差和$R^2$统计。我们从许多测量样本中推导出这些量，以便对模型精度进行统计上显着的验证。我们在第V，VI和VII节中的结果表明，一小部分测量数据足以满足模型的系统和工作负载特定系数。\n\nSpearman相关系数$\\rho$是预测因子和响应之间相关性的非参数度量。接近零的值表示弱相关性。当我们选择预测变量时，我们使用此统计量来验证假设。\n\n$R^2$ 统计量衡量模型与训练数据的拟合程度，$R^2$ 接近100％表示更好的拟合。RMS误差是预测误差或回归残差的标准偏差。它衡量密切观察的数据如何适应模型生成的数据预测。接近零的RMS误差值表示模型预测与观测数据之间的紧密拟合。\n\n$k$交叉验证方法将数据集划分为$k$个等大小的子集。$k-1$个子集用作模型训练数据，剩余子集是模型测试数据。我们重复这个过程来测试整个数据集的模型精度。我们使用3交叉验证来确认模型不会过度拟合数据的子集。\n\n#### 帕累托前沿评估\n\n 我们计算观察到的和预测的帕累托前沿之间的RMS误差，但也使用进一步的特定技术来评估预测前沿的准确性。我们使用以下指标评估帕累托前沿准确度：\n\n（1）重叠点数，即在观察和预测前沿中出现的点数;\n\n（2）观察前沿上的非重叠点数，按预测前沿距离最近邻居的距离分组;\n\n（3）预测前沿上的非重叠点数，按观察前沿距离最近邻居的距离分组;\n\n（4）预测的和观察的每个目标的最小值和最大值（能效或性能）;\n\n（5）预测的和观察的每个目标的权衡范围与线程和CPU频率处于最大设置时的值相比较。\n\n我们测量到最近邻居的距离占搜索空间维度的百分比。例如，我们每个节点维度的线程数有4个增量，从4到44，步长为4.从36到40的距离为1/11 = 9％，36到44为2/11 = 18％。此选择可以跨尺寸标准化。\n\n#### 测量和建模误差\n\n由于系统的非确定性，性能和功率测量表现出实验误差和噪声。这些测量误差通常是分布的，是由一系列随机因素引起的。正态性测试可以验证我们的功率和性能测量是否正常分布。我们使用Q-Q（分位数 - 分位数）图来直观地比较我们的测量数据分布与标准正态分布。我们使用$t$分布来分析测量置信区间。我们的模型使用的$log_e$变换减轻了正态分布偏差。\n\n模型预测表现出预测误差。因此，当我们确定性能和能效之间的权衡选择时，我们必须考虑测量和预测误差。错误限制内的有效权衡选项位于靠近帕累托前沿的区域，而不是仅直接位于其上。我们引入了权衡区（trade-off zone）的概念，其中包括帕累托前沿附近的所有值，这些值在统计上与前面的值无法区分。\n\n{% asset_img Figure3.png %}\n\n图1显示了我们构建权衡区的过程：\n\n(1) 根据权衡参数绘制数据集（在我们的例子中，能效和性能）;\n\n(2) 沿帕累托最优点绘制帕累托前沿;\n\n(3) 水平和垂直延伸帕累托前沿外部界限，以包含前面的在各自轴上误差范围内的点（我们还使用此曲线计算预测的帕累托前沿的与观察值的RMS误差）;\n\n(4) 通过轴误差限制来缩放和平移帕累托前沿，以设置权衡区的内部界限;\n\n(5) 关闭两条曲线，创建一个代表权衡区的多边形。\n\n当我们补偿每个轴的误差极限时，权衡区多边形包围可能是帕累托最优的点集。这些要点为能效和性能提供了权衡选择。由于测量和预测的误差限制可能不同，我们为测量和预测的Pareto前沿单独设置误差限制。\n\n我们的模型仅将测量数据用于响应变量，这提供了两个重要的好处。首先，我们的拟合模型系数不会受到响应测量中随机误差的影响。这意味着我们的回归估计倾向于训练数据的平均值，加上或减去测量误差。其次，我们的模型可用于对看不见的预测数据进行预测，因此我们实际上可以通过少量训练测量来探索大的参数空间。\n\n通过确保与程序初始化和关闭时间相比总体执行时间较长，并且与系统功率计数器的时间分辨率相比，执行时间较长，我们可以将测量误差水平控制在5％左右。在具有较大测量误差的较少受控环境中，随着更多数据点落入误差限制内，权衡区的大小将增加。搜索空间的完整实验测量扫描将受到增加的测量误差的类似影响。我们使用估计的测量误差和模型误差来评估观察到的和预测的权衡区域之间的对齐或重叠水平。\n\n## 实验概述\n\n#### 平台\n\n我们在Cray XC系统上进行实验，如表II所示。我们的运行最多使用86个，专门分配44个核心节点，或总共3,784个核心。我们使用Python编程平台[35]进行相关性分析，拟合模型系数，评估响应，并绘制结果。Nimrod工具包用于协调实验任务。\n\n{% asset_img Figure4.png %}\n\n#### 能耗测量\n\nCray XC系统具有节点级传感器，用于测量温度，电流和电压。Cray电源管理计数器（pm_counters）提供实时功率和能量测量，以10Hz的频率更新。CrayPAT是一种性能分析工具，它使用性能计数器（包括pm_counters和硬件性能计数器（HWPC））来评估程序行为。它检测程序，在运行时收集指定的计数器，并报告收集的计数器。当程序启动或终止时，也可以直接从Linux sysfs文件夹/sys/cray/pm_counters读取Cray pm_counters。我们使用pm_counters监控所有节点的能耗和功率。为了评估能效，我们使用每焦耳的操作数（例如，Flops/J，Bytes/J，Updates/J）。\n\n#### 参数配置\n\n表III列出了我们在研究中使用的配置参数和相关范围。这些参数生成具有484种组合的全因子设计。\n\n{% asset_img Figure5.png %}\n\n#### 实验\n\n我们在两个主要阶段进行研究实验：\n\n（1）使用程序内核进行模型设计和评估;\n\n（2）使用应用程序的模型评估。\n\n程序内科和应用程序都使用混合MPI/OpenMP编程模型。我们配置实验作业，为每个CPU插槽分配一个MPI Rank，为每个CPU核心分配一个OpenMP线程。我们使用Scatter线程放置策略在可用插槽和节点之间统一分发OpenMP线程。对于每个实验，我们收集表III中列出的完整484个样品组合，对于三个程序内核和两个应用程序总共进行2,420次测试。\n\n## 程序Kernel测试\n\n本节说明我们可以针对特定计算常用的几个科学内核以低成本准确地预测帕累托最优能量和性能权衡选项。我们选择Parallel Research Kernels（PRK）[1]，这包含着一系列程序，涵盖并行HPC应用程序中遇到的通信、计算和同步的常见模式。PRK具有性能指标报告，这使我们能够专注于功耗属性，以识别能源优化机会。在PRK中可用的混合MPI/OpenMP kernels中，我们专注于以下三个内核：\n\n（1）Stencil：一个内核，它执行数据并行模板操作到二维数组;\n\n（2）Transpose：一个强调通信和内存带宽的内核;\n\n（3）Nstream：一个易并行内核，它可以计算内存带宽。\n\n**这里我们以Stencil来说明本文方法的操作过程。**\n\n#### Stencil内核\n\n模板的模型响应项是MFlops/J中的能量效率和MFlops/s中的性能。我们使用模板半径2，网格大小400k，并设置迭代，以确保运行时间至少是测量采样频率的10倍。\n\n{% asset_img Figure6.png %}\n\n我们评估了几种生成模型训练数据的方法，包括均匀采样，随机采样和拉丁超立方采样。图2显示了我们的11核心数×11频率搜索空间，其中包含均匀或非随机样本、随机样本和拉丁超立方体样本，这些样本从每行和每列中随机选择样本。 \n\n我们使用RMS误差和 $R^2$ 统计数据评估每种采样方法，如第III-C节所述。我们使用统一采样在我们的测试案例中获得最少的观察结果。**我们的方法需要12个样本或10％的搜索空间进行模型训练（4个核心数 $\\times$ 3个频率样本）**。我们的核心数分别为8,20,32和44个核心，频率分别为1.2,1.8和2.2 GHz。我们还在节点数为20,42,64和86的情况下评估模型，总数据集大小为484个样本。\n\n{% asset_img Figure7.png %}\n\n图3包括95％ $t$-分布置信区间下5个样本平均值的误差条。对于我们的实验，测量误差范围约为5％。我们希望使用我们的模型设置工具将包括一个校准步骤，用于计算每个响应变量的测量置信区间。该策略消除了每个训练样本的样本重复，以计算其置信区间。相反，如果回归结果的统计显着性不在设定限度内，我们会提醒用户。\n\n{% asset_img Figure8.png %}\n\n图4显示了64个节点上模板的实验观察和模型预测Pareto前沿。前面的点不是帕累托最优，因为前面的点总是提供一个参数的改进而对另一个参数的影响较小。我们沿着测量和预测的帕累托前沿缩小了权衡区域，误差率为5％。\n\n线程数和频率之间的相互作用决定了帕累托前沿的形状。图4显示数据点按线程数量分组，并随着频率变化而旋转。这种旋转定义了帕累托前沿的形状，它设定了能量与性能的权衡范围。\n\n{% asset_img Figure9.png %}\n\n表IV显示了完整的121个观察值和预测值之间的RMS误差，能效为4.8％，性能为4.8％。帕累托前沿的RMS误差为能效效率的5.1％和性能的11.4％，我们按照第III-E节中的步骤3）进行计算。表IV中的Pareto Front部分显示了类似的观察和预测的Pareto点计数，除了一个重叠点（4个线程或0.1 GHz）的9％或一个搜索步骤之外的所有非重叠点。\n\n表IV还显示了最大核心和线程的 $Baseline$ 性能和能量效率，以及它们沿帕累托前沿的最小值和最大值，$P_{min}$ 和$P_{max}$。\n\n观察到的和预测的能效增益是相似的（约40％）。性能提升接近20％。观测和模型预测的Pareto前沿提供了一致的视图，核心和频率调整可以提供40％的能效增益，同时对性能的影响最小 。\n\n{% asset_img Figure10.png %}\n\n{% asset_img Figure11.png %}\n\n图5和图6中的表面表示在CPU频率和线程数搜索空间中观察到的和预测的能量效率和性能。我们在这些曲面上绘制Pareto点以在搜索空间中显示它们的上下文。\n\n图6显示随着频率和核心数量的增加，性能会提高，然后会趋于平稳。平衡标志着权衡区的开始，但是如图5所示，由于资源争用，能源效率在该位置显著下降。能效与性能之间的这种差异意味着，与仅最小化运行时间的策略相比，调整可显着提高能效。\n\n#### 真实应用测试\n\n实验中还选择了真实的并行应用程序，AMG，LAMMPS。对于AMG，能耗减少15%，性能损失20%；对于LAMMPS，能耗减少30%，但是性能损失30%。\n\n#### 3-交叉验证\n\n我们使用交叉验证来表明我们的模型不会过度拟合数据的子集。通过3交叉验证，我们将数据集随机分成三个大小相等的子集。两个子集用作模型训练数据，其余子集用作模型测试数据。我们重复该过程三次以测试整个数据集中的模型精度。\n\n{% asset_img Figure12.png %}\n\n表VIII显示了研究内核/应用的能效模型的交叉验证结果。每个内核的数据集有484个样本，因此数据集分为两个部分，一部分包含两fold，每fold中有161个样本，一部分包含一fold，共162个样本。我们显示每个fold的预测值百分比低于20％，10％和5％以及fold的RMS误差。跨越fold的相对均匀的RMS误差结果表明我们的能效模型不会过度拟合任何数据子集。\n\n{% asset_img Figure13.png %}\n\n表IX显示了使用相同数据集的性能模型的结果。整个fold的一致RMS误差结果表明，我们的性能模型也不会过度拟合任何数据子集。 \n\n## 参考\n\n[1] Parallel Research Kernels. https://github.com/ParRes/Kernels","tags":["Papers","Energy Modeling","SC"]},{"title":"论文阅读：Energy-Dfficient Application Resource Scheduling Using Machine Learning Classifiers (ICPP18)","url":"/2019/08/02/论文阅读：Energy-dfficient-Application-Resource-Scheduling-using-Machine-Learning-Classifiers-ICPP18/","content":"\n## 摘要\n\n高性能计算（HPC）中的资源调度通常旨在最小化应用程序运行时间而不是优化能量效率。大多数关于降低功率和能量消耗的现有研究强加了允许很少或没有性能损失的约束，这改善但仍然没有最大化能量效率。通过优化能量效率而不是应用执行时间，我们可以降低运行科学应用的成本。本文提出使用由低级硬件性能计数器驱动的机器学习分类来预测在应用程序运行时期间使用的最节能的资源设置，这与静态资源调度不同，动态地适应不断变化的应用程序行为。我们使用四种复杂的生物信息学HPC应用程序评估我们在大型共享内存系统上的方法，将对基本的Race调度的能量消耗平均降低20％，并最多时能达到38％。运行时间平均增加31％的代价之下是功耗降低了39％，我们从中推断出未来过度配置、功率受限的集群吞吐量将增加24％的可能性。这项工作证明了低开销分类方法适用于在应用程序运行时期间动态优化能量效率。\n\n## 研究背景\n\n在未来的百亿亿次系统中，能源和电力消耗将是首要的制约因素。例如，这些系统预计有严格的运行预算，如20MW。此外，E级操作系统需要有效地降低每一次科学计算的成本。目前已经存在大量研究工作，通过直接优化科学计算时间开销来最大化HPC的性能。但是最近的研究（理论和实践）证明，为应用程序分配最大的资源并没有比智能地根据特定应用程序的需求调整资源使用策略更加节能。因此，一些研究在保证应用程序性能的同时降低能耗，来进一步减少科学计算的开销。然而，目前很少有工作来积极地减少系统能耗。即使运行时间增加，也要最大限度地减少应用程序的能耗，可以大大降低科学洞察力的成本（以焦耳为单位），并在过度配置的功耗受限系统（例如未来E级计算机）中增加应用的吞吐量，允许更多应用程序同时在集群上执行。\n\n由于亿亿次级计算节点的复杂性和应用程序的动态特性，最大限度地减少E级系统的能耗，其资源需求在执行期间会有所不同。在节点内，可以调整许多不同的资源，这些资源会影响计算效率，包括动态电压和频率调整（DVFS），多插槽和核心分配以及HyperThreads的使用。这些资源的最节能配置因不同的应用程序而异，甚至在单个应用程序中也会因多个阶段的过渡而变化。 \n\n本文提出了再运行时动态监控应用程序和系统的方法，利用机器学习分类来预测应用程序执行时最节能的资源设置。该方法有两个不同的优点。首先，它可以应用于新的应用程序而无需修改分类器或应用程序，因为它不需要应用程序级别的插桩或挂钩。其次，分类开销低，使其适合在计算节点上原位运行。本文首先评估了15种不同的分类技术，并在四插槽x86计算节点上利用21个常用HPC基准测试和协同设计应用程序测试了他们在正确获取的节能配置的能力。实验发现，由于基础系统的复杂性使得许多分类方法不适用于预测节能配置的问题，但少数方法很有希望。然后，本文选择原有的15个分类技术中的5个，并评估它们动态管理四个高性能基因组装配应用程序的能力。这些代表了一些计算密集度最高的生物信息学应用程序，最常用的应用程序仅在单个共享内存节点上运行。因此，有一些HPC系统，如NERSC-Genepool [36]，主要用于同时执行许多单节点应用，如基因组装配和比较分析。\n\n本文的评估提供以下见解。首先，选择一个能够处理观察到的运行时行为与能量系统设置的复杂非线性映射的分类器非常重要。与维持最高性能所需的策略相比，较差的分类可以导致能量消耗的非平凡增加，例如，在我们探索的一个案例中，增加22％。然而，鉴于一个非常强大的分类，所提议的方法平均减少20％的能量消耗 - 只比离线Oracle高出5.4％的能耗。我们计算将此行为扩展到过度配置的功率受限群集可以将总吞吐量提高24％。所有这些结果都是在没有应用程序的先验知识且没有应用程序级别的修改的情况下实现的。我们得出结论，通过部署机器学习分类器来动态调整资源使用情况，可以显着提高每焦耳的科学洞察力（scientific insight）。\n\n综上所述，本文的贡献如下：\n\n（1）提出优化能源效率而不是运行时间，以降低科学计算的成本。\n（2）建立了15种不同的机器学习分类技术评估的问题复杂性，只有一些适用于优化能源效率。\n（3）通过在运行时准确预测能量系统设置，证明有足够强大的分类器可以大大降低能耗。\n\n## 如何最大化能源效率\n\n集群能效可简单地定义为***每焦耳能量完成的应用比率***。如果每个应用程序执行代表某些科学洞察力单元，则该指标表示在运营成本（能源消耗）方面的科学洞察力的成本。因此，通过最小化应用能耗，最大化能效直接降低了科学洞察力的成本。然而，减少能量（Energy）消耗比简单地减少电功率（Power）更困难，因为***能量是时间和功率的乘积***，并且很难在不牺牲对方的情况下改善能量。例如，提高处理器速度通常会缩短应用程序运行时间，但必要的功率增加会导致能耗增加。简而言之，最大化能源效率并不意味着尽可能少地使用电力 - 这需要在执行时间和功耗之间进行最佳折中。\n\n最近的能量感知方法在保持应用性能的同时降低了能量。由于这些方法不以减少应用程序的速度为代价，因此它们通过降低功率来严格降低能耗。虽然通过简单地消除应用程序运行时的约束来最小化能耗是很诱人的，但是如果不考虑低功率资源设置对执行时间的影响可能导致更差的能量消耗。\n\nHPC集群中的典型资源管理方法主要是尝试优化应用程序完成时间，仅考虑功率消耗以达到不违反总集群功率约束的程度，并完全忽略能耗。虽然有各种技术可以将作业分配给节点，但通常可以通过尽可能快地运行各个计算节点来实现最小化其运行时间 - 这种方法我们称之为Race。使用Race方法使节点上的资源调度变得容易，但它不像能够理解系统配置如何在每个应用程序基础上最性能和功率进行折中的更智能的方法那样节能。原因是尽可能快地投入工作可以最大限度地缩短执行时间，但功率的增加使节省的时间相形见绌，从而导致高能耗。\n\n除了降低成本之外，最大化能效还将提高功率受限环境中的整体系统吞吐量。在过度配置（Over-provisioned）的集群中，如果节点在Race设置中运行，硬件可以消耗比基础设施实际提供给系统更多的总功率。优化能源效率可以提高总体集群吞吐量：（1）允许更多应用程序并行运行，因为每个应用程序的功耗较低，而（2）由于其对能耗的影响仍在考虑应用程序运行时间。因此，需要新的资源管理方法，即使整体运行时间增加，也只关注最小化能耗以降低科学成本（以焦耳为单位）。\n\n## 学习能源效率\n\n识别能量有效的资源设置组合具有挑战性，因为系统没有通用的最佳设置 - 它取决于应用程序及其配置，甚至根据不同的输入而变化。即使在具有均匀节点和完全均匀的应用行为的并行系统中，由于制造变化，最佳设置也会发生显着变化。此外，许多应用程序在执行期间通过不同的阶段进行。例如，应用程序可能在计算和内存密集型处理之间进行转换，从而导致最节能的设置在运行时期间发生变化。因此，在启动应用程序时静态选择单个设置并不是最佳选择，而是需要采用动态方法。\n\n本文探索了调整系统设置以最小化应用能耗的学习方法。具体来说，我们调整插槽（Socket）分配，使用HyperThreads和处理器DVFS。学习组件为这些资源的组合选择设置，使得在应用程序执行期间的任何时刻，系统都在其最节能的状态下运行。\n\n本文的目标是让学习器在不需要任何应用程序级别更改的情况下进行最节能的设置。因此，我们使用现有的硬件性能计数器作为特征（Features）。然后，学习器确定相应的函数来将这些特征的一些子集映射到最节能系统设置。但是，现代计算节点中的系统设置具有非常复杂的交互关系，因此尽管存在这种复杂性，但使用能够生成准确映射的学习机制至关重要。 \n\n{% asset_img Figure1.png [20] [20]%}\n\n我们只使用两个特征 - 性能计数器POWER_DRAM（内存使用量度量）和EXEC（CPU使用量度量）来简单地说明这种复杂性。图1a显示了关于标准化性能计数器值的训练数据（21个常见HPC基准）的行为。每个数据点是在我们的评估系统上以单一资源配置运行的训练应用程序的平均记录POWER_DRAM和EXEC行为。有88种独特的资源配置或可能的标签，它们考虑了Socket计数S的不同组合，是否使用了HyperThreads HT，以及DVFS频率（例如2.1 GHz）。属于特定应用的所有88个数据点被分配相同的标签 - 具有该应用的最佳能量效率的资源配置;即，应用程序的所有88个数据点具有相同的颜色。通过这种标记，一个好的学习器将识别次优行为并产生最节能的设置来代替使用。***问题显然很复杂，并没有出现直观的模式将CPU和内存使用情况映射到最节能的系统设置中***。\n\n为了成功地将这些特征映射到准确的预测中，学习器必须能够处理这种复杂性，而不是所有机器学习机制都可以。考虑图1b表示使用线性内核的支持向量机（SVM）分类器的准确性。阴影区域表示SVM分类器预测的标签（系统设置），在训练集上的召回率仅为45.6%，这清楚地表明该分类器没有效果。相比之下，图1c展示了一个具有径向基函数（RBF）内核的SVM，它可以实现71.0％的召回率，但可能还有进一步改进的余地。\n\n## 系统设置的分类\n\n我们建议在运行时预测能量有效设置，而不是在产生结果之前估计所有可能系统设置的行为。与许多先前在HPC系统中管理功率/能量的工作一样，我们使用硬件性能计数器来测量应用和系统行为。\n\n{% asset_img Figure2.png [20] [20]%}\n\n图2展示了我们提出的方法。当应用程序在计算节点上运行时，硬件性能计数器会在后台定期轮询。对于我们的实验，我们使用PCM工具收集性能计数器数据。对数据进行缩放，然后使用主成分分析（PCA）进行处理，以确定哪些特征与能量效率相关。我们还使用特征选择来限制分类器使用的硬件计数器的数量，以减少运行时开销。该分类器预测要使用的最节能的设置，然后在系统上启动。该过程在下一个间隔重复进行。\n\n####  训练数据\n\n在使用之前必须训练一个分类器。为了收集训练数据，我们通过在所有可能的设置中运行基准应用程序并收集硬件性能计数器结果来描述基准应用程序在目标平台上的行为。换句话说，如果存在N个不同的可接受的设置，则每个应用程序执行N次，或者在每个设置中执行一次。对于M训练应用，训练集中有$N\\times M$个特征向量。\n\n训练数据的获取可能非常耗时，但只需要对每个平台进行一次，并且可以通过缩短应用程序执行时间来在合理的时间内完成。选择能够代表将在系统上使用的应用程序的基准测试程序可以提高分类器在运行时期间准确预测设置的可能性。此外，应将训练集中的基准测试程序以不同的模式运行,以涵盖硬件计数器的各种可能的用例。\n\n应用能源效率（EE）定义为每单位能源（J）完成的工作量。一般而言，在我们的评估中，完整的应用程序执行是完成工作的度量。与许多使用性能计数器的先前工作一样，我们的分类器需要一种措施来量化运行时的应用程序进度。底层硬件性能计数器没有用于量化真实应用程序进度的指标（已完成工作），但先前的工作已成功使用系统执行的指令数量（INST）。一些先前的工作甚至试图仅测量那些被认为在测量应用程序进度中有用的指令，例如，忽略自旋锁或并行化/同步指令。使用INST是优化总应用能源效率的不完美解决方案，但正如实验评估所展示的那样，已经足够了。然后，该类别使用以下公式作为量化训练过程中的能量效率： \n$$\nEE=\\frac{INST}{J}\\tag{1}\n$$\n对于M个应用程序中的每一个程序，使用该应用程序的最节能设置来标记N个特征向量。因此，分类器学习有效和低效的行为，以便在观察到类似的运行时行为时产生一个有效的预测：\n$$\nFeatVec_{mn}\\mapsto \\underset{i\\in N}{\\arg max EE_i},    \\forall m\\in M, \\forall n\\in N\\tag{2}\n$$\n对于应用程序执行，总指令计数不是固定的 : 计数通常随着应用程序执行时间而增加，例如，由于诸如PCM或内核任务的后台进程。因此，重要的是要注意使用公式2标记最节能的配置。在计算能量效率时使用指令计数有两个原因：首先，可以在执行期间的任何时候量化能量，使其成为运行时行为分析的有用指标。第二，更重要的是，公式1是当时发生的事件的函数。总能量需要知道在应用程序执行期间将发生的所有事件，这就引入了一种可能性，即分类器可能正在学习关于应用程序输入的一些知识，而不是硬件事件与能量消耗对应的方式。利用指令数量有助于避免这种陷阱。\n\n#### 性能计数器\n\n性能计数器指标反映了不同层次粒度，即系统、插槽和核心。为简单起见，我们限制为系统范围的数据。表1列出了我们实验中所选择的性能计数器。\n\n{% asset_img Figure3.png [20] [20]%}\n\n性能计数器也会转换为采样速率（根据需要），这是为了在不缩放值的情况下改变采样间隔所必需的。因为我们使用PCM来收集性能计数器指标，所以我们读取的硬件计数器比我们处理的更多（表1）。实际上，现成的解决方案只能通过读取和处理所使用的性能计数器来减少开销。大多数先前的工作都积极地限制它们访问的硬件计数器，以减少采样开销并减少其模型中的计算。\n\n\n\n## 实验设计\n\n本节描述了我们的实验设置，包括评估系统，用于训练和评估的应用程序以及测试的分类算法。我们对四插槽，80-物理核心系统进行评估，其中512 GB DRAM运行Ubuntu Linux 14.04 LTS，内核为4.4.0。使用HyperThreads，有160个计算线程可用，即每个插槽上有20个物理线程和20个虚拟线程。\n\n#### 用于训练的应用程序\n\n> NAS Parallel Benchmarks [4], Lawrence Liver\u0002more Lab’s Co-design benchmarks (AMG [16], Kripke [27], LULESH[23], Quicksilver [29]), and Argonne’s CESAR Proxy-apps (XSBench [48], RSBench [47]). Other applications include CoMD [28], Berke\u0002ley’s HPGMG-FV [2], a partial diferential equation solver (jacobi), and STREAM [33].  \n\n#### 用于评估的应用程序\n\n> HipMer [14], IDBA [41], Megahit [30] and metaSPAdes [37]. \n\n这四种应用也可以被认为是更广泛的HPC应用的代表。它们实现复杂的管道，具有多个不同的阶段，需要不同的资源调整才能有效地运行 - 有些是计算密集型的，有些是I / O密集型的，有些是通信密集型的。确切地说，使用哪些阶段以及它们对性能的贡献程度在很大程度上取决于程序的配置和输入数据集。尽管他们正在解决同样的问题，但它们的实现方式却各不相同，有不同的编程语言，不同的算法和不同的数据低点。例如，HipMer最多可以有20个不同的阶段，而Megahit可能只有几个阶段。总的来说，这些应用程序提供了一系列不同的生物信息学方法的广泛覆盖（频率计数，图形遍历，对齐，排序等）。因此，这些应用程序是我们使用分类在运行时预测适当设置的方法的主要候选者。\n\n***与训练应用程序一样，我们将每个评估应用程序配置为使用160个线程运行，但使用80个线程的metaSPAdes除外。在实验中我们固定了应用程序的输入和配置，但它们支持各种配置和输入，这些配置和输入会影响性能和能耗行为。*** 为了获取实验中的评估基准，我们在每个DVFS设置中运行这些程序来获取DVFS特征，其中分配了所有160个虚拟核心（对于metaSPAdes，所有80个物理核心用作基线）。从这些结果我们得到了一个DVFS Oracle，它知道，对于每个应用程序，能量最有效的静态DVFS频率 - 每个应用程序都有不同的最节能的静态设置。另请注意，此数据不用于训练分类器。\n\n#### 分类算法\n\n实验中选择了15中算法用于训练，各个算法得到的召回率如图4所示。根据预测效果，选择了5种算法，\n\n* ET：一个极随机的决策树，类似于随机森林。\n* GB：在偏差损失函数的负梯度上拟合多个回归树。\n* KNN：训练数据中最近邻居的简单多数投票（默认情况下，k = 5）。\n* MLP：神经网络使用lbfgs，tanh激活函数和四层优化对数损失函数。\n* SVM：使用径向基函数（RBF）内核的最大边际分类。\n\n{% asset_img Figure4.png [20] [20]%}\n\n## 实验评估\n\n#### 减少能耗开销\n\n论文的实验，主要评估了五种分类算法和使用数据库Oracle的性能，如图5所示，该方法能够显著减少能耗开销。\n\n{% asset_img Figure5.png [20] [20]%}\n\n#### 预测间隔和特征选择\n\n该部分评估了不同间隔下该方法的性能，以及选择不同的特征下方法的性能。\n\n{% asset_img Figure6.png [20] [20]%}\n\n{% asset_img Figure7.png [20] [20]%}\n\n#### 方法本身的开销\n\n论文也剖析了该方法各个部分的开销。\n\n{% asset_img Figure8.png [20] [20]%}\n\n#### 使用不同的分类器\n\n对于不同部分的预测，使用不同的分类器，实验表明，对于使用单一的分类器，有的程序能达到很好的效果，而使用不同的分类器，在有的程序上能达到很好的效果，但是有的程序上，能耗开销会很大。\n\n{% asset_img Figure9.png [20] [20]%}","tags":["Papers","Energy Modeling","ICPP"]},{"title":"HPL参数优化","url":"/2019/07/31/HPL参数优化/","content":"### 1. 实验平台KNL配置\nIntel Xeon Phi Processor 7210 ( 16GB, 1.30 GHz, 64 core )\nProcessor name : Intel(R) Xeon Phi(TM) 7210\nPackages (sockets) : 1\nCores : 64\nProcessors (CPUs) : 256\nCores per package : 64\nThreads per core : 4\n\nRAM: 96GB\nMCDRAM: 16 GB\n\n理论峰值\n**n 1*64*1.3*32=2662.4 Gflops**\n\n### 2. HPL.dat文件中需要优化的参数\n\n```Bash\nHPLinpack benchmark input file\nInnovative Computing Laboratory, University of Tennessee\nHPL.out      output file name (if any)\n6            device out (6=stdout,7=stderr,file)\n1            # of problems sizes (N)\n50000 100000 150000 200000  Ns\n10            # of NBs\n1 2 4 8 16 32 64 128 256 512     NBs\n0            PMAP process mapping (0=Row-,1=Column-major)\n1            # of process grids (P x Q)\n1            Ps\n1            Qs\n16.0         threshold\n3            # of panel fact\n0 1 2        PFACTs (0=left, 1=Crout, 2=Right)\n2            # of recursive stopping criterium\n2 4          NBMINs (>= 1)\n1            # of panels in recursion\n2            NDIVs\n3            # of recursive panel fact.\n0 1 2        RFACTs (0=left, 1=Crout, 2=Right)\n1            # of broadcast\n0            BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)\n1            # of lookahead depth\n0            DEPTHs (>=0)\n2            SWAP (0=bin-exch,1=long,2=mix)\n64           swapping threshold\n0            L1 in (0=transposed,1=no-transposed) form\n0            U  in (0=transposed,1=no-transposed) form\n1            Equilibration (0=no,1=yes)\n8            memory alignment in double (> 0)\n```\n\n***需要优化的主要有N，NB，PxQ等***\n\n### 3. 根据内存情况，获取理论最优的N值\n\n根据KNL的MCDRAM为16G，N*N*8=16G，得到N约为46000\n从46000左右开始设置N值，进行测试。\n\n* log信息提取命令\n\n```Bash\nawk -F: '/WR/' knl003_hpl.o31829 > 30000_128_8_8.log\n```\n* 阶段性结果\n\n|N|NB|Ps|Qs|Result（Gflops）|\n| ------ | ------ | ------ |--------|-----|\n|30000|\t128|\t8|\t16|\t9.435e+02|\n|30000\t|128\t|16\t|16\t|9.086e+01|\n|30000\t|256\t|8\t|16\t|6.042e+02|\n|35000\t|128\t|8\t|16\t|1.034e+03|\n|35000\t|64\t|8\t|16\t|8.820e+02|\n|35000\t|128\t|14\t|14\t|6.002e+02|\n|35000\t|128\t|1\t|128\t|6.019e+02|\n|35000\t|128\t|10\t|16\t|7.427e+02|\n|35000\t|128\t|8\t|8\t|1.149e+03|\n|35000\t|128\t|8\t|10\t|8.718e+02|\n|39200\t|128\t|8\t|8\t|1.227e+03|\n|39200\t|175\t|8\t|8\t|1.208e+03|\n|41600\t|128\t|8\t|8\t|1.074e+03|\n|32768\t|128\t|8\t|8\t|1.111e+03|\n\n\n更改运行脚本\n\n```Bash\n#!/usr/bin/bash\n \n \n#PBS -N knl003_hpl\n#PBS -l nodes=1,walltime=01:00:00\n \ncd /home/asc0146/haomeng/code/hpl/bin/Linux_Intel64/test1/\nexport KMP_AFFINITY=scatter,verbose\nexport OMP_NUM_THREADS=4\nexport MKL_NUM_THREADS=4\nmpiexec -np 64 ./xhpl | tee HPL.out\n```\n进行实验\n\n\n|N\t|NB|\tPs|\tQs|\tResult（Gflops）|\n|-----|-----|-----|-----|-----|\n|35000|\t128|\t8\t|8\t|效果很差|\n\n\n* 参数选择指导\n\n参考[Developer Guide for Intel](https://software.intel.com/en-us/mkl-linux-developer-guide-configuring-parameters).\nThe most significant parameters in HPL.dat are P, Q, NB, and N. Specify them as follows:\n\n  * P and Q - the number of rows and columns in the process grid, respectively.\nP*Q must be the number of MPI processes that HPL is using.\nChoose P ≤ Q.\n\n  * NB - the block size of the data distribution.\nThe table below shows recommended values of NB for different Intel® processors:\n{% asset_img HPL.png \"KNL推荐配置\" %}\n\n  * N - the problem size:\nFor homogeneous runs, choose N divisible by NB*LCM(P,Q), where LCM is the least common multiple of the two numbers.\nFor heterogeneous runs, see Heterogeneous Support in the Inte；l Optimized MP LINPACK Benchmark for how to choose N.\n\n***NOTE***\n\nIncreasing N usually increases performance, but the size of N is bounded by memory. In general, you can compute the memory required to store the matrix (which does not count internal buffers) as 8*N*N/(P*Q) bytes, where N is the problem size and P and Q are the process grids in HPL.dat. A general rule of thumb is to choose a problem size that fills 80% of memory. When offloading to Intel Xeon Phi coprocessors, you may choose a problem size that fills 70% of memory, to leave room for additional buffers needed for offloading. Choose N and NB such that N > > NB.\n\n* 最新优化情况\n\n    * 优化结果\n\n|N|\tNB|\tPs|\tQs\t|Result（Gflops）|\n|----|----|----|-----|-----|\n|3920\t|128\t|8\t|16\t|9.435e+02|\n","tags":["并行"]},{"title":"HPC性能评测","url":"/2019/07/31/HPC性能评测/","content":"高性能计算集群的性能评测大致分为机器级、算法级和程序级的性能评测。\n\n* 机器级\n\n\n机器级的性能评测主要包括CPU和存储器的某些基本性能指标、并行通讯开销以及机器的成本、价格和性/价比，有些是由厂商在销售时直接提供给用户的，是广大用户对并行计算机的第一印象，是引进和购买并行计算机时最主要的选择依据。\n\n\n\n* 算法级\n\n算法级的性能评测主要包括加速、效率和可扩展性等，最初是为了评价并行算法的性能提出的，用在并行机性能评测主要是测并行系统的加速比和可扩展性。并行系统的加速比是指对于一个给定的应用，并行算法（或并行程序）的执行速度相对于串行算法的执行速度加快了多少倍。***可扩展性是在确定的应用背景下，并行系统性能随处理器数的增加而按比例提高的能力***。\n\n* 程序级\n\n程序级的性能评测主要是使用一组基准测试来测试和评价并行系统的各种性能，包括基本测试、数学库测试和并行测试程序等。\n","tags":["并行"]},{"title":"并行程序性能分析软件（Extra-P和Score-P）安装过程","url":"/2019/07/31/并行程序性能分析软件（Extra-P和Score-P）安装过程/","content":"### Extra-P配置安装\n\n#### 安装位置\n* 实验室台式机虚拟机Ubuntukylin17.10\n\n#### 软件包依赖\n\n* QT4 或 QT5\n* Cube（>=4.3）\n* Python3\n* PyQT\n* MatPlotlib\n\n#### 安装过程\n\n* 安装Qt4或Qt5\n\n```Bash\nsudo apt-get install qt4-default\nsudo apt-get install qt5-default\nsudo apt-get install qt4-dev-tools\nsudo apt-get install qt5-dev-tools\n```\n\n* 安装python3\n\n```Bash\nsudo apt-get install python3\n```\n\n* 安装PyQt\n\n```Bash\nsudo apt-get install pyqt4-dev-tools\nsudo apt-get install pyqt5-dev-tools\n```\n\n* 安装Cube \n\n下载Cube源码包cube-4.3.tar.gz\n\n```Bash\n./configure\nmake\nmake all\n```\n\n使用QT5的话编译过程中会出现一些问题，选用Qt4能正常编译\n\n* 安装 MatPlotLib\n\n```Bash\nsudo apt-get install python3-pip\nsudo python -m pip install -U pip\nsudo python -m pip install -U matplotlib\n```\n\n#### 编译Extra-P\n\n```Bash\nmkdir build\ncd build\n../configure --with-cube=/opt/cube CPPFLAGS=\"-I/usr/include/python3.6m/\"\nmake\nmake install\n```\n\n### score-P配置安装\n\n```Bash\nmkdir build\ncd build\n../configure \nmake\nmake install\n```\n","tags":["并行"]},{"title":"LLVM入门学习","url":"/2019/07/31/LLVM入门学习/","content":"### 1. LLVM各工具使用\n\n* Convert C to IR\n\n```\nclang -emit-llvm -S multiply.c -o multiply.ll\n```\n\nOr\n\n```\nclang -cc1 -emit-llvm testfile.c -o testfile.ll\n```\n\nOr\n\n```\nclang test.c -S -emit-llvm -o test.ll\n```\n\n\n\n* Convert IR to bc\n\n```\nllvm-as test.ll –o test.bc\n```\n\n\n\n* Converting LLVM bitcode to target machine assembly\n\n```\nllc test.bc –o test.s\n```\n\nOr \n\n```\nclang -S test.bc -o test.s\n```\n\n\n\n* Converting bc to IR\n\n```\nllvm-dis test.bc –o test.ll\n```\n\n\n\n* Transforming LLVM IR\n\n```\nopt –passname input.ll –o output.ll\n```\n\n\n\n* Linking llvm bc\n\n```\nllvm-link test1.bc test2.bc –o output.bc\n```\n\n\n\n* Executing llvm bc\n\n```\nlli output.bc\n```\n\n\n\n\n\n### 2. IRBuilder\n\n可以批量的插入指令\n\n* 在之前前面插入指令\n\n```\nInstruction *pi = ...;\nIRBuilder<> Builder(pi);\nCallInst* callOne = Builder.CreateCall(...);\nCallInst* callTwo = Builder.CreateCall(...);\nValue* result = Builder.CreateMul(callOne, callTwo);\n```\n\n* 在基本块后面插入指令\n\n```\nBasicBlock *pb = ...;\nIRBuilder<> Builder(pb);\nCallInst* callOne = Builder.CreateCall(...);\nCallInst* callTwo = Builder.CreateCall(...);\nValue* result = Builder.CreateMul(callOne, callTwo);\n```\n\n但IRBuilder方法对于常数的指令会被优化掉\n\n\n\n### 3. 遍历Funciton或BasicBlock\n\n除了使用\n\n```\nfor(auto F=M.begin(),E=M.end();F!=E;F++)\n```\n\n还可以使用\n\n```\nfor(Module::iterator F = M.begin(), E = M.end();F!=E;++F)\n```\n\n注意，这块要使用比要得到的高一级的迭代器，即如果得到Function，则使用Module::iterator;\n\n如果得到BasicBlock，则使用Function::iterator\n\n如果遍历Instruction，可使用BasicBlock::iterator,或\n\n如果遍历Instruction的参数，可使用Instrunction::op_iterator\n\n* Function 中有一个函数size()，可以得到该函数中基本块的数量\n\n\n\n### 4. LLvm编译DEBUG版本的命令\n\n```\nexport CC=gcc\n\nexport CXX=g++\n\n./configure --prefix=/home/haomeng/.local --sysconfdir=/etc --enable-shared --enable-libffi --enable-targets=all --disable-expensive-checks --disable-assertions --with-binutils-include=/usr/include --with-python=/usr/bin/python2 --disable-optimized --enable-debug-runtime\n\nmake REQUIRES_RTTI=1 -j 60\n\nmake install\n\n```\n\n### 5. Pass\n\n在LLVM框架中，Pass用于对LLVM IR进行优化，对IR进行处理与分析，生成优化后的代码。`opt`命令可以用来运行pass对IR进行处理。\n\n优化类型\n\n* opt 自带的优化\n\n步骤如下：\n\n```\nclang -S -O0 -emit-llvm example.c\n\nopt -O0 -S example.ll\n\nopt -O1 -S example.ll\n\nopt -O2 -S example.ll\n\nopt -O3 -S example.ll\n\n为了能够看到opt所调用的优化Pass，可以加入参数\n\n--debug-pass=Structure\n\n```\n\n参考：http://llvm.org/docs/CommandGuide/opt.html \n\n\n\n* 自己写Pass进行优化\n\n参考：http://llvm.org/docs/WritingAnLLVMPass.html\n\n\n\n* 在一个Pass里面调用别的Pass\n\ngetAnalysis函数\n\n\n\n* 通过Pass manager 对Pass进行管理\n\n\n\n* Analysis Pass 分析IR但不对IR进行修改，其结果可以在多个Pass中使用，直到IR改变\n\n没有更改IR内容就返回false\n\n### 6. LLVM的Use-Def获取\nAlternatively, it’s common to have an instance of the User Class and need to know what Values are used by it. The list of all Values used by a User is known as a use-def chain. Instances of class Instruction are common User s, so we might want to iterate over all of the values that a particular instruction uses (that is, the operands of the particular Instruction):\n```\nInstruction *pi = ...;\n \nfor (Use &U : pi->operands()) {\n  Value *v = U.get();\n  // ...\n}\n\n```\n\n### 7. LLVM中Def-Use获取\nFrequently, we might have an instance of the Value Class and we want to determine which Users use the Value. The list of all Users of a particular Value is called a def-use chain. \n\n* 函数的def-use\n\n```\nFunction *F = ...;\n \nfor (User *U : F->users()) {\n  if (Instruction *Inst = dyn_cast<Instruction>(U)) {\n    errs() << \"F is used in instruction:\\n\";\n    errs() << *Inst << \"\\n\";\n  }\n```\n或者\n```\nFunction *F = ...;\nfor (Value::use_iterator U = F->use_begin(), e = F->use_end(); U != e; ++U) {\n  if (Instruction *Inst = dyn_cast<Instruction>(&*(U->getUser()))) {\n    errs() << \"F is used in instruction:\\n\";\n    errs() << *Inst << \"\\n\";\n}\n```\n\n* 指令的Def-Use\n```\nInstruction *A = ...;\n \nfor (User *U : A->users()) {\n  if (Instruction *Inst = dyn_cast<Instruction>(U)) {\n    errs() << \"A is used in instruction:\\n\";\n    errs() << *Inst << \"\\n\";\n  }\n```\n或\n```\nInstruction *A = ...;\nfor (Value::use_iterator i = A->use_begin(), e = A->use_end(); i != e; ++i) {\n  if (Instruction *U = dyn_cast<Instruction>(&*(i->getUser()))) {\n  ...\n  }\n}\n```\n\n### 8. 遍历LLVM IR指令的操作数\n\n通过User中提供的op_iterator迭代器来遍历Instruction中的操作数\n```\nInstruction* V = ...\nfor (User::op_iterator op = V->op_begin(), e = V->op_end(); op != e; ++op){\n    if (Instruction *U = dyn_cast<Instruction>(op->get())) {\n...\n    }\n}\n```\n\n### 9. 获取Basic Block的前驱基本块\nLLVM已经提供了遍历基本块的所有前驱的函数\n```\nBasicBlock* B = ...\nfor (auto it = pred_begin(B), et = pred_end(B); it != et; ++it)\n{\n  BasicBlock* predecessor = *it;\n  ...\n}\n```","tags":["LLVM"]},{"title":"物理CPU和逻辑CPU的区别","url":"/2019/07/31/物理CPU和逻辑CPU的区别/","content":"A physical core is what it sounds like - an actual physical processor core in your CPU. Each physical core has its own circuitry and its own L1 (and usually L2) cache can read and execute instructions separately (for the most part) from the other physical cores on the chip.\n\nA logical core is more of a programming abstraction than an actual physical entity. A simple definition of a logical core is that it is a processing unit that is capable of executing its own thread in parallel with other logical cores. In fact you could say that a logical core is the same as a thread.\n\nYou can have multiple logical cores per physical core. However logical cores share resources with other logical cores operating on the same physical core, so having more logical cores will not necessarily get you the same performance increase as having more physical cores.\n\nIn the case of intel hyperthreading (HT), you have two logical cores per physical core, so a quad-(physical) core i7 processor will have eight logical cores. However the two logical cores within one physical core cannot truly operate in parallel with respect to each other. This is because HT works by having one logical core operate while the other logical core is waiting and has nothing to do (for example when it is waiting on a cache or memory fetch).\n\nWell then how can these logical cores be considered in parallel? Well most of the time they can be because during typical CPU operation you will almost never see continuous execution of a single thread on every clock cycle - there are always gaps when one logical core is waiting for something and the second logical core can kick in and do its job.","tags":["CPU"]},{"title":"PAPI安装问题","url":"/2019/07/31/PAPI安装问题/","content":"### 解决方法\nIt seems kernel didn't let me access to performance events because of security reasons. You have to set that.\n(由于安全原因，似乎内核不允许访问性能事件,必须对它进行设置)\n\n```shell\nsudo sh -c 'echo -1 >/proc/sys/kernel/perf_event_paranoid'\n```\n","tags":["RAPL","PAPI"]},{"title":"PAPI安装RAPL模块","url":"/2019/07/31/PAPI安装RAPL模块/","content":"### Configuring PAPI to support RAPL\nWhen configuring PAPI, you need to provide the --with-components=rapl option. Basically, you need to install PAPI like this:\n```\n$ tar xzvf papi-5.3.2.tar.gz\n$ cd papi-5.3.2/src\n$ ./configure --with-components=rapl &&make &&make install\n```\n### Checking the PAPI installation\nOnce PAPI is installed, make sure it is capable of reading RAPL information. For instance, you can run this [rapl-read](http://eztrace.gforge.inria.fr/tutorials/tutorial_rapl/rapl-read.tgz) program. You may need to modify the `Makefile` in order to specify the installation directory of PAPI. Once compiled, running the `rapl-read` program should result in the following output:\n\n```Bash\n$ ./rapl_plot\nFound rapl component at cid 2\nFound: rapl:::THERMAL_SPEC_CNT:PACKAGE0\nFound: rapl:::MINIMUM_POWER_CNT:PACKAGE0\nFound: rapl:::MAXIMUM_POWER_CNT:PACKAGE0\nFound: rapl:::MAXIMUM_TIME_WINDOW_CNT:PACKAGE0\nFound: rapl:::PACKAGE_ENERGY_CNT:PACKAGE0\nFound: rapl:::PP1_ENERGY_CNT:PACKAGE0\nFound: rapl:::DRAM_ENERGY_CNT:PACKAGE0\nFound: rapl:::PP0_ENERGY_CNT:PACKAGE0\nFound: rapl:::THERMAL_SPEC:PACKAGE0\nFound: rapl:::MINIMUM_POWER:PACKAGE0\nFound: rapl:::MAXIMUM_POWER:PACKAGE0\nFound: rapl:::MAXIMUM_TIME_WINDOW:PACKAGE0\nFound: rapl:::PACKAGE_ENERGY:PACKAGE0\nFound: rapl:::PP1_ENERGY:PACKAGE0\nFound: rapl:::DRAM_ENERGY:PACKAGE0\nFound: rapl:::PP0_ENERGY:PACKAGE0\n[...]\n4.7213 0.0 (* Average Power for rapl:::MAXIMUM_TIME_WINDOW:PACKAGE0 *)\n4.7213 3.0 (* Average Power for rapl:::PACKAGE_ENERGY:PACKAGE0 *)\n4.7213 0.2 (* Average Power for rapl:::PP1_ENERGY:PACKAGE0 *)\n4.7213 0.8 (* Average Power for rapl:::DRAM_ENERGY:PACKAGE0 *)\n4.7213 0.1 (* Average Power for rapl:::PP0_ENERGY:PACKAGE0 *)\n4.8218 0.0 (* Average Power for rapl:::THERMAL_SPEC_CNT:PACKAGE0 *)\n4.8218 0.0 (* Average Power for rapl:::MINIMUM_POWER_CNT:PACKAGE0 *)\n4.8218 0.0 (* Average Power for rapl:::MAXIMUM_POWER_CNT:PACKAGE0 *)\n4.8218 0.0 (* Average Power for rapl:::MAXIMUM_TIME_WINDOW_CNT:PACKAGE0 *)\n4.8218 0.0 (* Average Power for rapl:::PACKAGE_ENERGY_CNT:PACKAGE0 *)\n4.8218 0.0 (* Average Power for rapl:::PP1_ENERGY_CNT:PACKAGE0 *)\n4.8218 0.0 (* Average Power for rapl:::DRAM_ENERGY_CNT:PACKAGE0 *)\n4.8218 0.0 (* Average Power for rapl:::PP0_ENERGY_CNT:PACKAGE0 *)\n4.8218 46156882941.9 (* Average Power for rapl:::THERMAL_SPEC:PACKAGE0 *)\n4.8218 0.0 (* Average Power for rapl:::MINIMUM_POWER:PACKAGE0 *)\n4.8218 0.0 (* Average Power for rapl:::MAXIMUM_POWER:PACKAGE0 *)\n4.8218 0.0 (* Average Power for rapl:::MAXIMUM_TIME_WINDOW:PACKAGE0 *)\n4.8218 2.9 (* Average Power for rapl:::PACKAGE_ENERGY:PACKAGE0 *)\n4.8218 0.2 (* Average Power for rapl:::PP1_ENERGY:PACKAGE0 *)\n4.8218 0.8 (* Average Power for rapl:::DRAM_ENERGY:PACKAGE0 *)\n```\n### Can't open fd for cpu0: No such file or director\nHowever, You may have the following error:\n```\n$ ./rapl_plot\nFound rapl component at cid 2\nNo rapl events found: Can't open fd for cpu0: No such file or director\n```\nThis usually means that the msr kernel module that permits to read the energy counters is not loaded. This should be fixed by running modprobe:\n```\n$ sudo modprobe msr\n```\n### Can't open fd for cpu0: Operation not permitted\nAnother possible error is:\n```\n$  ./rapl_plot\nFound rapl component at cid 2\nNo rapl events found: Can't open fd for cpu0: Operation not permitted\n```\nIn that case, you may have to run the program as sudo:\n```\n$ sudo ./rapl_plot\n[...]\n```\n","tags":["RAPL"]},{"title":"RAPL介绍","url":"/2019/07/31/RAPL介绍/","content":"### 介绍RAPL的链接 \n\n1. RUNNING AVERAGE POWER LIMIT， https://01.org/zh/blogs/2014/running-average-power-limit-%E2%80%93-rapl?langredirect=1 \n\n2. Reading RAPL energy measurements from Linux, http://web.eece.maine.edu/~vweaver/projects/rapl/index.html \n \n3. Energy measurements in Linux，https://blog.chih.me/read-cpu-power-with-RAPL.html \n \n4. Intel® Power Governor, https://software.intel.com/en-us/articles/intel-power-governor\n","tags":["RAPL"]},{"title":"OpenMP编译成LLVM IR","url":"/2019/07/29/OpenMP编译成LLVM-IR/","content":"\nhttps://clang-omp.github.io/\n\n# 介绍\n\n目前clang/llvm编译器已经支持OpenMP，目前，OpenMP 3.1已经被clang/llvm 3.7完全支持。\n\n# 方法\n\n这是一个openmp程序\n\n```\n\n#include <omp.h>\n\n#include <stdio.h>\n\nint main() {\n\n#pragma omp parallel\n\nprintf(\"Hello from thread %d, nthreads %d\\n\", omp_get_thread_num(), omp_get_num_threads());\n\n}\n\n```\n\n将更改程序转化为相应的IR代码：\n\n```\n\nclang -fopenmp -emit-llvm -S test.c -o test.ll\n\n```\n\nIR代码如下：\n\n```\n\n; ModuleID = 'test.bc'\n\ntarget datalayout = \"e-m:e-i64:64-f80:128-n8:16:32:64-S128\"\n\ntarget triple = \"x86_64-pc-linux-gnu\"\n\n%ident_t = type { i32, i32, i32, i32, i8* }\n\n@.str = private unnamed_addr constant [35 x i8] c\"Hello from thread %d, nthreads %d\\0A\\00\", align 1\n\n@.str.1 = private unnamed_addr constant [23 x i8] c\";unknown;unknown;0;0;;\\00\", align 1\n\n@0 = private unnamed_addr constant %ident_t { i32 0, i32 2, i32 0, i32 0, i8* getelementptr inbounds ([23 x i8], [23 x i8]* @.str.1, i32 0, i32 0) }, align 8\n\n; Function Attrs: nounwind uwtable\n\ndefine i32 @main() #0 {\n\n  call void (%ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%ident_t* @0, i32 0, void (i32*, i32*, ...)* bitcast (void (i32*, i32*)* @.omp_outlined. to void (i32*, i32*, ...)*))\n\n  ret i32 0\n\n}\n\n; Function Attrs: nounwind uwtable\n\ndefine internal void @.omp_outlined.(i32* noalias %.global_tid., i32* noalias %.bound_tid.) #0 {\n\n  %1 = alloca i32*, align 8\n\n  %2 = alloca i32*, align 8\n\n  store i32* %.global_tid., i32** %1, align 8\n\n  store i32* %.bound_tid., i32** %2, align 8\n\n  %3 = call i32 @omp_get_thread_num()\n\n  %4 = call i32 @omp_get_num_threads()\n\n  %5 = call i32 (i8*, ...) @printf(i8* getelementptr inbounds ([35 x i8], [35 x i8]* @.str, i32 0, i32 0), i32 %3, i32 %4)\n\n  ret void\n\n}\n\ndeclare i32 @printf(i8*, ...) #1\n\ndeclare i32 @omp_get_thread_num() #1\n\ndeclare i32 @omp_get_num_threads() #1\n\ndeclare void @__kmpc_fork_call(%ident_t*, i32, void (i32*, i32*, ...)*, ...)\n\nattributes #0 = { nounwind uwtable \"disable-tail-calls\"=\"false\" \"less-precise-fpmad\"=\"false\" \"no-frame-pointer-elim\"=\"true\" \"no-frame-pointer-elim-non-leaf\" \"no-infs-fp-math\"=\"false\" \"no-nans-fp-math\"=\"false\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+fxsr,+mmx,+sse,+sse2\" \"unsafe-fp-math\"=\"false\" \"use-soft-float\"=\"false\" }\n\nattributes #1 = { \"disable-tail-calls\"=\"false\" \"less-precise-fpmad\"=\"false\" \"no-frame-pointer-elim\"=\"true\" \"no-frame-pointer-elim-non-leaf\" \"no-infs-fp-math\"=\"false\" \"no-nans-fp-math\"=\"false\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+fxsr,+mmx,+sse,+sse2\" \"unsafe-fp-math\"=\"false\" \"use-soft-float\"=\"false\" }\n\n!llvm.ident = !{!0}\n\n!0 = !{!\"clang version 3.8.0-2ubuntu4 (tags/RELEASE_380/final)\"}\n\n```\n\n其中***mpc_fork_call***是OpenMP定义的运行时函数，用来生成线程池，每个线程将执行函数***omp_outlined***中的代码，该函数的代码就是对应并行域中的代码。\n\n\n{% asset_img example1.png [20] [10]%}\n\nOver!!!!\n","tags":["LLVM"]},{"title":"LLVM Pass工程建立过程","url":"/2019/07/29/LLVM-Pass工程建立过程/","content":"一般的Pass工程建立方法需要利用LLVM源码目录进行编译，本文介绍了另一种Pass工程建立和编译方法，与cmake相结合进行Pass编译。\n### 1. 环境安装\n* 安装LLVM和clang编译器\n\n```Bash\nsudo apt-get install clang\nsudo apt-get install clang++\nsudo apt-get install llvm\n```\n### 2. LLVM Pass工程目录\n该测试工程LLVM_Test主要包含两个文件夹cmake和src，其具体文件组织形式如下图所示。**需要注意的是，利用该工程编译Pass时只需要在src目录下添加相应的Pass源码即可，其他文件不需要修改**。\n{% asset_img example1.png %}\n\n\n具体内容如下：\n* cmake目录中的FindLLVM.cmake\n\n```Bash\n# - Find LLVM \n# This module can be used to find LLVM.\n# It requires that the llvm-config executable be available on the system path.\n# Once found, llvm-config is used for everything else.\n#\n# The following variables are set:\n#\n# LLVM_FOUND                 - Set to YES if LLVM is found.\n# LLVM_VERSION               - Set to the decimal version of the LLVM library.\n# LLVM_INCLUDE_DIRS          - A list of directories where the LLVM headers are located.\n# LLVM_LIBRARY_DIRS          - A list of directories where the LLVM libraries are located.\n# LLVM_LIBRARIES             - A list of libraries which should be linked\n# LLVM_DYNAMIC_LIBRARY       - A single dynamic llvm shared library\n# LLVM_DYNAMIC_LIBRARY_FOUND - Whether found the dynamic llvm shared library\n# LLVM_OPT                   - opt program in llvm\n# \n# Using Following macros to set static library:\n# llvm_map_components_to_libraries(OUTPUT_VARIABLE ${llvm components})\n# \n# tutorial:\n#   1.  select default LLVM version:\n#       cmake .. -DLLVM_RECOMMEND_VERSION=\"3.5\"\n#   2.  set include dir and link dir:\n#       include_directories(${LLVM_INCLUDE_DIRS})\n#       link_directories(${LLVM_LIBRARY_DIRS})\n#   3.a link static libraries:\n#       llvm_map_components_to_libraries(LLVM_IRREADER_LIRARY irreader)\n#       target_link_libraries(target\n#           ${LLVM_LIBRARIES}\n#           ${LLVM_IRREADER_LIRARY}\n#           )\n#   3.b link a dynamic library:\n#       target_link_libraries(target ${LLVM_DYNAMIC_LIBRARY}) \n#\n# 14-10-26: \n#    LLVM_RECOMMAND_VERSION --> LLVM_RECOMMEND_VERSION\n#    update tutorial\n# \n# version: 0.9.1\n#    add LLVM_FLAGS_NDEBUG means llvm build with NDEBUG\n#\n# version: 0.9\n#    remove LLVM_{C/CPP/CXX}_FLAGS which import -DNDEBUG\n#\n#\nif(NOT DEFINED LLVM_RECOMMEND_VERSION)\n   set(LLVM_RECOMMEND_VERSION \"\" CACHE STRING \"Switch the llvm version\")\n   set_property(CACHE LLVM_RECOMMEND_VERSION PROPERTY STRINGS \"\" \"3.4\" \"3.5\" \"3.9\")\nendif()\n\n\nif(NOT(DEFINED LLVM_ROOT) )\n\tif(NOT \"${LLVM_VERSION}\" EQUAL \"{LLVM_RECOMMEND_VERSION}\")\n\t\tunset(LLVM_CONFIG_EXE CACHE)\n\t\tunset(LLVM_DYNAMIC_LIBRARY CACHE)\n\tendif()\n\t# find llvm-config. perfers to the one with version suffix, Ex:llvm-config-3.2\n\tfind_program(LLVM_CONFIG_EXE NAMES \"llvm-config-${LLVM_RECOMMEND_VERSION}\" \"llvm-config\")\n   find_program(LLVM_OPT NAMES \"opt-${LLVM_RECOMMEND_VERSION}\" \"opt\")\n\n\tif(NOT LLVM_CONFIG_EXE)\n\t\tset(LLVM_FOUND False)\n      message(FATAL_ERROR \"Not Found LLVM (LLVM_RECOMMEND_VERSION=${LLVM_RECOMMEND_VERSION})\")\n\telse()\n\t\tset(LLVM_FOUND True)\n\tendif()\n\n\t# Get the directory of llvm by using llvm-config. also remove whitespaces.\n\texecute_process(COMMAND ${LLVM_CONFIG_EXE} --prefix OUTPUT_VARIABLE LLVM_ROOT\n\t\tOUTPUT_STRIP_TRAILING_WHITESPACE )\n\nendif()\n\nmacro(_llvm_config _var_name)\n    execute_process(COMMAND ${LLVM_CONFIG_EXE} ${ARGN} \n        OUTPUT_VARIABLE ${_var_name}\n        OUTPUT_STRIP_TRAILING_WHITESPACE\n        )\nendmacro()\n\nset(LLVM_INSTALL_PREFIX  ${LLVM_ROOT})\nadd_definitions(-D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS)\n\n_llvm_config(LLVM_VERSION --version)\nSTRING(REGEX REPLACE \"^([0-9]+)\\\\.[0-9]+(svn)?\\\\.?[0-9]*\" \"\\\\1\" LLVM_VERSION_MAJOR \"${LLVM_VERSION}\")\nSTRING(REGEX REPLACE \"^[0-9]+\\\\.([0-9]+)(svn)?\\\\.?[0-9]*\" \"\\\\1\" LLVM_VERSION_MINOR \"${LLVM_VERSION}\")\n_llvm_config(LLVM_LD_FLAGS --ldflags)\n_llvm_config(LLVM_LIBRARY_DIRS --libdir)\n_llvm_config(LLVM_INCLUDE_DIRS --includedir)\nstring(REGEX MATCH \"-l.*\" LLVM_LIBRARIES ${LLVM_LD_FLAGS})\n_llvm_config(LLVM_C_FLAGS --cflags)\nif(LLVM_C_FLAGS MATCHES \"-DNDEBUG\")\n   add_definitions(-DLLVM_FLAGS_NDEBUG)\nendif()\n\nfind_library(LLVM_DYNAMIC_LIBRARY \n\tNAMES \"LLVM\" \"LLVM-${LLVM_VERSION}\"\n   PATHS ${LLVM_LIBRARY_DIRS}\n   )\n\nif(NOT LLVM_DYNAMIC_LIBRARY)\n\tset(LLVM_DYNAMIC_LIBRARY_FOUND False)\nelse()\n\tset(LLVM_DYNAMIC_LIBRARY_FOUND True)\nendif()\n\nmacro(llvm_map_components_to_libraries _var_name)\n    _llvm_config(${_var_name} --libs \"${ARGN}\")\nendmacro()\n\nmessage(STATUS \"Found LLVM Version ${LLVM_VERSION} \")\n\n```\n* 主目录的CMakeLists.txt\n\n```\ncmake_minimum_required(VERSION 2.8)\nproject(Test)\n\nset(CMAKE_MODULE_PATH \n   ${CMAKE_MODULE_PATH}\n   ${CMAKE_SOURCE_DIR}/cmake\n   )\n\nif(NOT CMAKE_BUILD_TYPE)\n   set(CMAKE_BUILD_TYPE \"Release\")\nendif()\n\nfind_package(LLVM)\n\nadd_subdirectory(src)\n```\n* src目录中的CMakeLists.txt\n\n```\naux_source_directory(. DIR_SRCS)\ninclude_directories(\n\t${LLVM_PROF_INCLUDE_DIRS}\n\t${PROJECT_BINARY_DIR}\n\t${LLVM_INCLUDE_DIRS} \n   ../include\n\t)\nlink_directories(\n   ${LLVM_LIBRARY_DIRS} \n   )\n\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wall --std=c++11 -fno-rtti\")\nset(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE} -DNO_DEBUG\")\nset(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG}\")\n\nadd_library(Test SHARED\n\t${DIR_SRCS}\n\t)\n\ntarget_link_libraries(Test\n\t${LLVM_DYNAMIC_LIBRARY}\n\t)\n\n```\n* src中的FunctionTest.cpp文件是LLVM Pass的一个示例，在该目录中可以编写相应的Pass\n\n```c++\n#include <llvm/Pass.h>\n#include <llvm/IR/Function.h>\n#include <llvm/IR/Module.h>\n#include <llvm/IR/Instructions.h>\n#include <llvm/IR/Constants.h>\n#include <llvm/Support/raw_ostream.h>\n#include <llvm/IR/InstIterator.h>\n#include <llvm/IR/Operator.h>\n#include <llvm/Analysis/AliasAnalysis.h>\nusing namespace llvm;\nnamespace{\n    class MyTest:public FunctionPass\n    {\n        public:\n            static char ID;\n            MyTest():FunctionPass(ID){}\n            bool runOnFunction(Function &F) override;\n    };\n}\nchar MyTest::ID = 0;\nstatic RegisterPass<MyTest> X(\"MyTest\",\"My Test\",false,false);\nbool MyTest::runOnFunction(Function &F) {\n\n    Function *tmp = &F;\n    errs()<<tmp->getName()<<\"\\n\";\n    // 遍历函数中的所有基本块\n    for (Function::iterator bb = tmp->begin(); bb != tmp->end(); ++bb) {\n        // 遍历基本块中的每条指令\n        for (BasicBlock::iterator inst = bb->begin(); inst != bb->end(); ++inst) {\n            if (inst->isBinaryOp()) {\n\t\t\terrs()<<*inst<<\"\\n\";\n            }\n        }\n    }\n\n    return false;\n}\n\n```\n\n### 3. 编译过程\n\n```\ncd LLVM_Test/\nmkdir build\ncd build/\ncmake ..\nmake\n```\n编译出来的.so文件在LLVM_Test/build/src目录下。具体过程如下图所示：\n{% asset_img example2.png %}\n\n### 4. Pass调用\n利用该工程编译之后，我们就可以调用相应的Pass了，具体调用过程如下：\n\n```\nopt -load LLVM_Test/build/src/libTest.so -MyTest test.ll \n```\n","tags":["LLVM"]}]